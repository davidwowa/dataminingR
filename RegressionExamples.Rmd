---
title: "Linear Regression Prestige Data, Example from DZone Refcardz"
output: html_notebook
---
Common Information:
1. Is for numeric values
2. Linear regression is based on the assumption that a linear relationship exists between the input and output variable.
3. Learning Alg. will learn the set of parameters such that the sum of square error(yactual - yestimate)2 is minimized.

Load Prestige Data:
```{r}
library(car)
```
```{r}
Prestige
```
```{r}
summary(Prestige)
```
```{r}
pairs(main="Prestige Data", Prestige)
```
```{r}
plot.ts(main="Prestige Data", Prestige)
```
In correlation matrix we see, that education and income in relation to prestige standing. 
```{r}
d <- data.frame(education = Prestige$education, income = Prestige$income, women = Prestige$women, prestige = Prestige$prestige, census = Prestige$census)
m <- cor(d)
library('corrplot')
corrplot(m, method="circle")
```
Education and income has a high influence to the prestige !!!

                                      Example 1 (Prestige).
####
Prepare Data Prestige:
Build index which later use only 1/4 from data as Training.
```{r}
testidx_pre <- which(1:nrow(Prestige)%%4==0)
```
Split data:
```{r}
prestige_train <- Prestige[-testidx_pre,]
```
```{r}
prestige_test <- Prestige[testidx_pre,]
```
Build a learn model: (for prestige attribute)
```{r}
model <- lm(prestige~., data=prestige_train)
```
Use model for prediction the test data
```{r}
prediciton <- predict(model, newdata=prestige_test)
```
Check correlation with current result, but why ? 
```{r}
cor(prediciton, prestige_test$prestige)
```
Show result:
```{r}
summary(model)
```
```{r}
curve(4.201e+00*x + 1.150e-03,from=1, to=100, xlab="x", ylab="y")
```
```{r}
plot(model)
```
```{r}
prediciton
```

                                      Example 2 (Census).
####
Prepare Data Prestige/Census:
Build index which later use only 1/4 from data as Training.
```{r}
testidx_pre <- which(1:nrow(Prestige)%%4==0)
```
Split data:
```{r}
prestige_train <- Prestige[-testidx_pre,]
```
```{r}
prestige_test <- Prestige[testidx_pre,]
```
Build a learn model: (for census attribute)
```{r}
model <- lm(census~., data=prestige_train)
```
Use model for prediction the test data
```{r}
prediciton <- predict(model, newdata=prestige_test)
```
Check correlation with current result, but why ? 
```{r}
cor(prediciton, prestige_test$census)
```
Show result:
```{r}
summary(model)
```
```{r}
curve(-5.135e+03*x + -3.377e+03,from=1, to=10000, xlab="x", ylab="y")
```
```{r}
plot(model)
```
```{r}
prediciton
```

                                      Example 3 (Prestige).
####
Prepare Data Prestige:
Build index which later use only 1/4 from data as Training.
```{r}
testidx_pre <- which(1:nrow(Prestige)%%10==0)
```
Split data:
```{r}
prestige_train <- Prestige[-testidx_pre,]
```
```{r}
prestige_test <- Prestige[testidx_pre,]
```
Build a learn model: (for prestige attribute)
```{r}
model <- lm(prestige~., data=prestige_train)
```
Use model for prediction the test data
```{r}
prediciton <- predict(model, newdata=prestige_test)
```
Check correlation with current result, but why ? 
```{r}
cor(prediciton, prestige_test$prestige)
```
Show result:
```{r}
summary(model)
```
```{r}
curve(4.201e+00*x + 1.150e-03,from=1, to=100, xlab="x", ylab="y")
```
```{r}
plot(model)
```
```{r}
prediciton
```

                                      Example 4 (Census).
####
Prepare Data Prestige/Census:
Build index which later use only 1/4 from data as Training.
```{r}
testidx_pre <- which(1:nrow(Prestige)%%10==0)
```
Split data:
```{r}
prestige_train <- Prestige[-testidx_pre,]
```
```{r}
prestige_test <- Prestige[testidx_pre,]
```
Build a learn model: (for census attribute)
```{r}
model <- lm(census~., data=prestige_train)
```
Use model for prediction the test data
```{r}
prediciton <- predict(model, newdata=prestige_test)
```
Check correlation with current result, but why ? 
```{r}
cor(prediciton, prestige_test$census)
```
Show result:
```{r}
summary(model)
```
```{r}
curve(-4.292e+03*x + -2.592e+03,from=1, to=10000, xlab="x", ylab="y")
```
```{r}
plot(model)
```
```{r}
prediciton
```

Columns with not marked with least one * can be safely ignored.

END

Goal of minimizing the square error makes linear regression very sensitive to outliers that greatly deviate in the output.